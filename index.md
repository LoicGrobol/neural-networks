---
title: R√©seau de neurones ‚Äî‚ÄØM2 PluriTAL 2022
layout: default
---

<!-- LTeX: language=fr -->

## News

- **2023-01-16** Les [consignes pour les projets]({{site.url}}{{site.baseurl}}/projects) sont en
  ligne.
- **2022-11-21** Premier cours du semestre le 23/11/2022

## Infos pratiques

- **Quoi** ¬´‚ÄØR√©seaux de neurones‚ÄØ¬ª
- **O√π** Salle 219, b√¢timent Paul Ric≈ìur
- **Quand** 8 s√©ances, les mercredi de 9:30 √† 12:30, du 23/11/22 au 25/01/23
  - Voir le planning pour les dates exactes (quand il aura √©t√© mis en ligne)
- **Contact** Lo√Øc Grobol [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)


**Note** ce cours remplace pour l'ann√©e 2022-2023 le cours ¬´‚ÄØarbres, graphes, r√©seaux‚ÄØ¬ª

## Liens utiles

- Lien Binder de secours‚ÄØ:
  [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/LoicGrobol/neural-networks/main)
- [Consignes pour les projets]({{site.url}}{{site.baseurl}}/project) sont en ligne.

## S√©ances

Tous les supports sont sur [github](https://github.com/loicgrobol/neural-networks), voir
[Utilisation en local](#utilisation-en-local) pour les utiliser sur votre machine comme des
notebooks. √Ä d√©faut, ce sont des fichiers Markdown assez standards, qui devraient se visualiser
correctement sur la plupart des plateformes (mais ne seront pas dynamiques).

Les slides et les notebooks ci-dessous ont tous des liens Binder pour une utilisation interactive
sans rien installer. Les slides ont aussi des liens vers une version HTML statique utile si Binder
est indisponible.

### 2022-11-23 ‚Äî Historique et perceptron simple

- {% notebook_badges slides/01-perceptron/perceptron-slides.py.md %}
  [Notebook perceptron simple](slides/01-perceptron/perceptron-slides.py.ipynb)

### 2022-12-07 - Perceptron multi-couches

- {% notebook_badges slides/02-nn/nn-slides.py.md %}
  [Notebook perceptron multi-couches](slides/02-nn/nn-slides.py.ipynb)

### 2023-01-03 ‚Äî Transformers

- {% notebook_badges slides/03-transformers/transformers-slides.py.md %}
  [Notebook demo](slides/03-transformers/transformers-slides.py.ipynb)
- [Cours ü§ó Transformers](https://huggingface.co/course)

### 2023-01-10 ‚Äî Descente de gradient et *backpropagation*

Un peu de lecture suppl√©mentaire‚ÄØ:

- Pour les maths et les visuels, la le√ßon [*backpropagation
  calculus*](https://www.3blue1brown.com/lessons/backpropagation-calculus) de 3blue1brown est bien.
- Pour l'historique, l'article [*Backpropagation*](https://en.wikipedia.org/wiki/Backpropagation) de
  Wikipedia en est pas mal.
- Pour une zoologie des variantes de l'algo de descente de gradient, [Sebastian
  Ruder](https://ruder.io/optimizing-gradient-descent/) a fait une bonne synth√®se

### 2023-01-18 ‚Äî Mod√®les de g√©n√©ration de s√©quences

Articles cit√©s‚ÄØ:

<!-- LTeX: language=en-US -->

- Adelani, David, Jesujoba Alabi, Angela Fan, Julia Kreutzer, Xiaoyu Shen, Machel Reid, Dana Ruiter,
  et al. 2022. ‚Äú[A Few Thousand Translations Go a Long Way! Leveraging Pre-Trained Models for
  African News Translation.](https://doi.org/10.18653/v1/2022.naacl-main.223)‚Äù In Proceedings of the
  2022 Conference of the North American Chapter of the Association for Computational Linguistics:
  Human Language Technologies, 3053‚Äì70. Seattle, United States: Association for Computational
  Linguistics.
- Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2015. ‚Äú[Neural Machine Translation by Jointly
  Learning to Align and Translate.](http://arxiv.org/abs/1409.0473)‚Äù In Proceedings of the 3rd
  International Conference on Learning Representations, edited by Yoshua Bengio and Yann LeCun. San
  Diego, California, USA.
- Fan, Angela, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep
  Baines, et al. 2021. ‚ÄúBeyond English-Centric Multilingual Machine Translation.‚Äù The Journal of
  Machine Learning Research 22 (1): 107:4839-107:4886.
- Levy, Omer, Kenton Lee, Nicholas FitzGerald, and Luke Zettlemoyer. 2018. ‚Äú[Long Short-Term Memory
  as a Dynamically Computed Element-Wise Weighted Sum.](http://arxiv.org/abs/1805.03716)‚Äù
  ArXiv:1805.03716 [Cs, Stat], May.
- Lewis, Mike, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
  Veselin Stoyanov, and Luke Zettlemoyer. 2020. ‚Äú[BART: Denoising Sequence-to-Sequence Pre-Training
  for Natural Language Generation, Translation, and
  Comprehension.](https://doi.org/10.18653/v1/2020.acl-main.703)‚Äù In Proceedings of the 58th Annual
  Meeting of the Association for Computational Linguistics, 7871‚Äì80. Online: Association for
  Computational Linguistics.
- Merrill, William, Gail Weiss, Yoav Goldberg, Roy Schwartz, Noah A. Smith, and Eran Yahav. 2020.
  ‚Äú[A Formal Hierarchy of RNN Architectures.](https://www.aclweb.org/anthology/2020.acl-main.43)‚Äù In
  Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 443‚Äì59.
  Online: Association for Computational Linguistics.
- Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
  Kaiser, and Illia Polosukhin. 2017. ‚Äú[Attention Is All You
  Need.](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)‚Äù
  In Advances in Neural Information Processing Systems 30, edited by I. Guyon, U. V. Luxburg, S.
  Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 5998‚Äì6008. Long Beach, California:
  Curran Associates, Inc..
- Weiss, Gail, Yoav Goldberg, and Eran Yahav. 2018. ‚Äú[On the Practical Computational Power of Finite
  Precision RNNs for Language Recognition.](https://www.aclweb.org/anthology/P18-2117)‚Äù, 2:740‚Äì45.
  Melbourne, Australia: Association for Computational Linguistics.

<!-- LTeX: language=fr -->

### 2023-01-25 ‚Äî Biais, attaques, donn√©es

Articles cit√©s‚ÄØ:

<!-- LTeX: language=en-US -->

- Abid, Abubakar, Maheen Farooqi, and James Zou. 2021a. ‚ÄúLarge Language Models Associate Muslims
  with Violence.‚Äù Nature Machine Intelligence 3 (6): 461‚Äì63.
  <https://doi.org/10.1038/s42256-021-00359-2>.
- Abid, Abubakar, Maheen Farooqi, and James Zou. 2021b. ‚ÄúPersistent Anti-Muslim Bias in Large
  Language Models.‚Äù In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society,
  298‚Äì306. AIES ‚Äô21. New York, NY, USA: Association for Computing Machinery.
  <https://doi.org/10.1145/3461702.3462624>.
- Adelani, David, Jesujoba Alabi, Angela Fan, Julia Kreutzer, Xiaoyu Shen, Machel Reid, Dana Ruiter,
  et al. 2022. ‚ÄúA Few Thousand Translations Go a Long Way! Leveraging Pre-Trained Models for African
  News Translation.‚Äù In Proceedings of the 2022 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies, 3053‚Äì70. Seattle, United
  States: Association for Computational Linguistics.
  <https://doi.org/10.18653/v1/2022.naacl-main.223>.
- Artetxe, Mikel, and Holger Schwenk. 2019. ‚ÄúMassively Multilingual Sentence Embeddings for
  Zero-Shot Cross-Lingual Transfer and Beyond.‚Äù Transactions of the Association for Computational
  Linguistics 7 (September): 597‚Äì610. <https://doi.org/10.1162/tacl_a_00288>.
- Bender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. ‚ÄúOn the
  Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú.‚Äù In Proceedings of the 2021 ACM
  Conference on Fairness, Accountability, and Transparency, 610‚Äì23. FAccT ‚Äô21. New York, NY, USA:
  Association for Computing Machinery. <https://doi.org/10.1145/3442188.3445922>.
- Birhane, Abeba. 2022. ‚ÄúChatGPT, Galactica, and the Progress Trap.‚Äù Wired, December 9, 2022.
  <https://www.wired.com/story/large-language-models-critique/>.
- Caliskan, Aylin, Joanna J. Bryson, and Arvind Narayanan. 2017. ‚ÄúSemantics Derived Automatically
  from Language Corpora Contain Human-like Biases.‚Äù Science 356 (6334): 183‚Äì86.
  <https://doi.org/10.1126/science.aal4230>.
- Cheng, Pengyu, Weituo Hao, Siyang Yuan, Shijing Si, and Lawrence Carin. 2022. ‚ÄúFairFil:
  Contrastive Neural Debiasing Method for Pretrained Text Encoders.‚Äù In .
  <https://openreview.net/forum?id=N6JECD-PI5w>.
- Fan, Angela, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep
  Baines, et al. 2021. ‚ÄúBeyond English-Centric Multilingual Machine Translation.‚Äù The Journal of
  Machine Learning Research 22 (1): 107:4839-107:4886.
- Gonen, Hila, and Yoav Goldberg. 2019. ‚ÄúLipstick on a Pig: Debiasing Methods Cover up Systematic
  Gender Biases in Word Embeddings But Do Not Remove Them.‚Äù In Proceedings of the 2019 Workshop on
  Widening NLP, 60‚Äì63. Association for Computational Linguistics.
  <https://www.aclweb.org/anthology/W19-3621/>.
- Gonen, Hila, Yova Kementchedjhieva, and Yoav Goldberg. 2019. ‚ÄúHow Does Grammatical Gender Affect
  Noun Representations in Gender-Marking Languages?‚Äù In Proceedings of the 23rd Conference on
  Computational Natural Language Learning (CoNLL), 463‚Äì71. Association for Computational
  Linguistics. <https://doi.org/10.18653/v1/K19-1043>.
- Gonen, Hila, Shauli Ravfogel, and Yoav Goldberg. 2022. ‚ÄúAnalyzing Gender Representation in
  Multilingual Models.‚Äù arXiv. <https://doi.org/10.48550/arXiv.2204.09168>.
- Gonen, Hila, and Kellie Webster. 2020. ‚ÄúAutomatically Identifying Gender Issues in Machine
  Translation Using Perturbations.‚Äù In Findings of the Association for Computational Linguistics:
  EMNLP 2020, 1991‚Äì95. Online: Association for Computational Linguistics.
  <https://doi.org/10.18653/v1/2020.findings-emnlp.180>.
- Kreutzer, Julia, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh,
  Allahsera Tapo, et al. 2022. ‚ÄúQuality at a Glance: An Audit of Web-Crawled Multilingual Datasets.‚Äù
  Transactions of the Association for Computational Linguistics 10 (January): 50.
  <https://doi.org/10.1162/tacl_a_00447>.
- Liang, Sheng, Philipp Dufter, and Hinrich Sch√ºtze. 2020. ‚ÄúMonolingual and Multilingual Reduction
  of Gender Bias in Contextualized Representations.‚Äù In Proceedings of the 28th International
  Conference on Computational Linguistics, 5082‚Äì93. Barcelona, Spain (Online): International
  Committee on Computational Linguistics. <https://doi.org/10.18653/v1/2020.coling-main.446>.
- Lucy, Li, and David Bamman. 2021. ‚ÄúGender and Representation Bias in GPT-3 Generated Stories.‚Äù In
  Proceedings of the Third Workshop on Narrative Understanding, 48‚Äì55. Virtual: Association for
  Computational Linguistics. <https://doi.org/10.18653/v1/2021.nuse-1.5>.
- Mikolov, Tomas, Wen-tau Yih, and Geoffrey Zweig. 2013. ‚ÄúLinguistic Regularities in Continuous
  Space Word Representations.‚Äù In Proceedings of the 2013 Conference of the North American Chapter
  of the Association for Computational Linguistics: Human Language Technologies, 746‚Äì51. Atlanta,
  Georgia: Association for Computational Linguistics. <https://aclanthology.org/N13-1090>.
- Perez, F√°bio, and Ian Ribeiro. 2022. ‚ÄúIgnore Previous Prompt: Attack Techniques For Language
  Models.‚Äù arXiv. <https://doi.org/10.48550/arXiv.2211.09527>.
- Perrigo, Billy. 2023. ‚ÄúThe $2 Per Hour Workers Who Made ChatGPT Safer.‚Äù Time, January 18, 2023.
  <https://time.com/6247678/openai-chatgpt-kenya-workers/>.
- Prabhakaran, Vinodkumar, Margaret Mitchell, Timnit Gebru, and Iason Gabriel. 2022. ‚ÄúA Human
  Rights-Based Approach to Responsible AI.‚Äù arXiv. <https://doi.org/10.48550/arXiv.2210.02667>.
- Prost, Flavien, Nithum Thain, and Tolga Bolukbasi. 2019. ‚ÄúDebiasing Embeddings for Reduced Gender
  Bias in Text Classification.‚Äù In Proceedings of the First Workshop on Gender Bias in Natural
  Language Processing, 69‚Äì75. Florence, Italy: Association for Computational Linguistics.
  <https://doi.org/10.18653/v1/W19-3810>.
- Ravfogel, Shauli, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. 2020. ‚ÄúNull It Out:
  Guarding Protected Attributes by Iterative Nullspace Projection.‚Äù In Proceedings of the 58th
  Annual Meeting of the Association for Computational Linguistics, 7237‚Äì56. Online: Association for
  Computational Linguistics. <https://doi.org/10.18653/v1/2020.acl-main.647>.
- Steed, Ryan, Swetasudha Panda, Ari Kobren, and Michael Wick. 2022. ‚ÄúUpstream Mitigation Is  NOt
  All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models.‚Äù In Proceedings
  of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
  Papers), 3524‚Äì42. Dublin, Ireland: Association for Computational Linguistics.
  <https://doi.org/10.18653/v1/2022.acl-long.247>.
- Wallace, Eric, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. ‚ÄúUniversal
  Adversarial Triggers for Attacking and Analyzing NLP.‚Äù In Proceedings of the 2019 Conference on
  Empirical Methods in Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing, 2153‚Äì62. Association for Computational Linguistics.
  <https://doi.org/10.18653/v1/D19-1221>.
- Weidinger, Laura, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra
  Cheng, et al. 2021. ‚ÄúEthical and Social Risks of Harm from Language Models.‚Äù arXiv.
  <https://doi.org/10.48550/arXiv.2112.04359>.


<!-- LTeX: language=fr -->


## Lire les slides en local

Les supports de ce cours sont √©crits en Markdown, convertis en notebooks avec
[Jupytext](https://github.com/mwouts/jupytext). C'est entre autres une fa√ßon d'avoir un historique
git propre, malheureusement √ßa signifie que pour les ouvrir en local, il faut installer les
extensions ad√©quates. Le plus simple est le suivant

1. R√©cup√©rez le dossier du cours, soit en t√©l√©chargeant et d√©compressant
   [l'archive](https://github.com/LoicGrobol/neural-networks/archive/refs/heads/main.zip)
   soit en le clonant avec git‚ÄØ: `git clone
   https://github.com/LoicGrobol/neural-networks.git` et placez-vous dans ce dossier.
2. Cr√©ez un environnement virtuel pour le cours

   ```console
   python3 -m virtualenv .venv
   source .venv/bin/activate
   ```

3. Installez les d√©pendances

   ```console
   pip install -U -r requirements.txt
   ```

4. Lancez Jupyter

   ```console
   jupyter notebook
   ```

   JupyterLab est aussi utilisable, mais la fonctionnalit√© slide n'y fonctionne pas pour l'instant.

## Ressources

### Apprentissage artificiel

- [*Speech and Language Processing*](https://web.stanford.edu/~jurafsky/slp3/) de Daniel Jurafsky et
  James H. Martin est **indispensable**. Il est disponible gratuitement, n'h√©sitez pas √† le
  consulter tr√®s fr√©quemment.
- [*Apprentissage artificiel - Concepts et
  algorithmes*](https://www.eyrolles.com/Informatique/Livre/apprentissage-artificiel-9782416001048/)
  d'Antoine Cornu√©jols et Laurent Miclet. Plus ancien mais en fran√ßais et une r√©f√©rence tr√®s
  compl√®te sur l'apprentissage (en particulier non-neuronal). Il est un peu cher alors si vous
  voulez l'utiliser, commencez par me demander et je vous pr√™terai le mien.

### Python g√©n√©ral

Il y a beaucoup, beaucoup de ressources disponibles pour apprendre Python. Ce qui suit n'est qu'une
s√©lection.

#### Livres

Ils commencent √† dater un peu, les derniers gadgets de Python n'y seront donc pas, mais leur lecture
reste tr√®s enrichissante (les algos, √ßa ne vieillit jamais vraiment).

- *How to think like a computer scientist*, de Jeffrey Elkner, Allen B. Downey, and Chris Meyers.
  Vous pouvez l'acheter. Vous pouvez aussi le lire
  [ici](http://openbookproject.net/thinkcs/python/english3e/)
- *Dive into Python*, by Mark Pilgrim. [Ici](http://www.diveintopython3.net/) vous pouvez le lire ou
  t√©l√©charger le pdf.
- *Learning Python*, by Mark Lutz.
- *Beginning Python*, by Magnus Lie Hetland.
- *Python Algorithms: Mastering Basic Algorithms in the Python Language*, par Magnus Lie Hetland.
  Peut-√™tre un peu costaud pour des d√©butants.
- Programmation Efficace. Les 128 Algorithmes Qu'Il Faut Avoir Compris et Cod√©s en Python au Cours
  de sa Vie, by Christoph D√ºrr and Jill-J√™nn Vie. Si le cours vous para√Æt trop facile. Le code
  Python est clair, les difficult√©s sont comment√©es. Les algos sont tr√®s costauds.

#### Web

Il vous est vivement conseill√© d'utiliser un (ou plus) des sites et tutoriels ci-dessous.

- **[Real Python](https://realpython.com), des cours et des tutoriels souvent de tr√®s bonne qualit√©
  et pour tous niveaux.**
- [Un bon tuto NumPy](https://cs231n.github.io/python-numpy-tutorial/) qui va de A √† Z.
- [Coding Game](https://www.codingame.com/home). Vous le retrouverez dans les exercices
  hebdomadaires.
- [Code Academy](https://www.codecademy.com/fr/learn/python)
- [newcoder.io](http://newcoder.io/). Des projets comment√©s, commencer par 'Data Visualization'
- [Google's Python Class](https://developers.google.com/edu/python/). Guido a travaill√© chez eux.
  Pas [ce
  Guido](http://vignette2.wikia.nocookie.net/pixar/images/1/10/Guido.png/revision/latest?cb=20140314012724),
  [celui-l√†](https://en.wikipedia.org/wiki/Guido_van_Rossum#/media/File:Guido_van_Rossum_OSCON_2006.jpg)
- [Mooc Python](https://www.fun-mooc.fr/courses/inria/41001S03/session03/about#). Un mooc de
  l'INRIA, carr√©.
- [Code combat](https://codecombat.com/)

### Divers

- La cha√Æne YouTube [3blue1brown](https://www.youtube.com/c/3blue1brown) pour des vid√©os de maths
  g√©n√©rales.
- La cha√Æne YouTube de [Freya Holm√©r](https://www.youtube.com/c/Acegikmo) plut√¥t orient√©e *game
  design*, mais avec d'excellentes vid√©os de g√©om√©trie computationnelle.

## Licences

[![CC BY Licence
badge](https://i.creativecommons.org/l/by/4.0/88x31.png)](http://creativecommons.org/licenses/by/4.0/)

Copyright ¬© 2022 Lo√Øc Grobol [\<loic.grobol@gmail.com\>](mailto:loic.grobol@gmail.com)

Sauf indication contraire, les fichiers pr√©sents dans ce d√©p√¥t sont distribu√©s selon les termes de
la licence [Creative Commons Attribution 4.0
International](https://creativecommons.org/licenses/by/4.0/). Voir [le README](README.md#Licences)
pour plus de d√©tails.

 Un r√©sum√© simplifi√© de cette licence est disponible √†
 <https://creativecommons.org/licenses/by/4.0/>.

 Le texte int√©gral de cette licence est disponible √†
 <https://creativecommons.org/licenses/by/4.0/legalcode>
