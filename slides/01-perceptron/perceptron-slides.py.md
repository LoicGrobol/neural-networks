---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.14.1
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- LTeX: language=fr -->
<!-- #region slideshow={"slide_type": "slide"} -->
Cours 01: Le perceptron simple
==============================

**LoÃ¯c Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

<!-- #endregion -->

```python
from IPython.display import display, Markdown
```

```python
import numpy as np
import matplotlib.pyplot as plt
```

## Principe et historique

[![SchÃ©ma d'un neurone avec des lÃ©gendes pour les organelles et les connexions importantes pour la
communication entre
neurones.](https://upload.wikimedia.org/wikipedia/commons/1/10/Blausen_0657_MultipolarNeuron.png)](https://commons.wikimedia.org/w/index.php?curid=28761830)

Un modÃ¨le de neurone biologique (plutÃ´t sensoriel)â€¯: une unitÃ© qui reÃ§oit plusieurs entrÃ©es $x_i$
scalaires (des nombres quoi), en calcule une somme pondÃ©rÃ©e $z$ (avec des poids $w_i$ prÃ©dÃ©finis) et
renvoie une sortie binaire $y$ ($1$ si $z$ est positif, $0$ sinon).


Autrement dit

$$\begin{align}
z &= \sum_i w_ix_i = w_1 x_1 + w_2 x_2 + â€¦ + w_n x_n\\
y &=
    \begin{cases}
        1 & \text{si $z > 0$}\\
        0 & \text{sinon}
    \end{cases}
\end{align}$$

FormulÃ© cÃ©lÃ¨brement par McCulloch et Pitts (1943) avec des notations diffÃ©rentes

**Attention** selon les auteurices, le cas $z=0$ est traitÃ© diffÃ©remment, pour *Speech and Language
Processing*, par exemple, on renvoie $0$ dans ce cas, c'est donc la convention qu'on suivra, mais
vÃ©rifiez Ã  chaque fois.


On peut ajouter un terme de *biais* en fixant $x_0=1$ et $w_0=b$, ce qui donne

$$\begin{equation}
    z = \sum_{i=0}^n w_ix_i = \sum_{i=1}^n w_ix_i + b
\end{equation}$$

Ou schÃ©matiquement


![](figures/perceptron/perceptron.svg)


Ou avec du code

```python
def perceptron(inpt, weights):
    """Calcule la sortie du perceptron dont les poids sont `weights` pour l'entrÃ©e `inpt`
    
    EntrÃ©esâ€¯:
    
    - `inpt` un tableau numpy de dimension $n$
    - `weights` un tableau numpy de dimention $n+1$
    
    Sortie: un tableau numpy de type boolÃ©en et de dimensions $0$
    """
    return (np.inner(weights[1:], inpt) + weights[0]) > 0
```

ImplÃ©mentÃ© comme une machine, le perceptron Mark I, par Rosenblatt (1958)â€¯:

[![Une photographie en noir et blanc d'une machine ressemblant Ã  une grande armoire pleine de fils
Ã©lectriques](https://upload.wikimedia.org/wikipedia/en/5/52/Mark_I_perceptron.jpeg)](https://en.wikipedia.org/wiki/File:Mark_I_perceptron.jpeg)


**Est-ce que Ã§a vous rappelle quelque choseâ€¯?**


ğŸ¤”


C'est un **classifieur linÃ©aire** dont on a dÃ©jÃ  parlÃ© dans le cours prÃ©cÃ©dent.


Les ambitions initiales Ã©taient grandes

> *the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.*  
> New York Times, rapportÃ© par Olazaran (1996)


C'est par exemple assez facile de construire un qui rÃ©alise l'opÃ©ration logique Ã©lÃ©mentaire $\operatorname{ET}$Â :

```python
and_weights = np.array([-0.6, 0.5, 0.5])
print("x\ty\tx ET y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = perceptron([x_i, y_i], and_weights).astype(int)
        print(f"{x_i}\t{y_i}\t{out}")
```

Ã‡a marche bien parce que c'est un problÃ¨me **linÃ©airement sÃ©parable**â€¯: si on reprÃ©sente $x$ et $y$
dans le plan, on peut tracer une droite qui sÃ©pare la parties oÃ¹ $x\operatorname{ET}y$ vaut $1$ et
la partie oÃ¹ Ã§a vaut $0$â€¯:

```python
import tol_colors as tc

x = np.array([0, 1])
y = np.array([0, 1])
X, Y = np.meshgrid(x, y)
Z = np.logical_and(X, Y)

fig = plt.figure(dpi=200)

heatmap = plt.scatter(X, Y, c=Z, cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```

Ici voilÃ  les valeurs que renvoie notre neuroneâ€¯:

```python
import tol_colors as tc

x = np.linspace(0, 1, 1000)
y = np.linspace(0, 1, 1000)
X, Y = np.meshgrid(x, y)
Z = 0.5*X + 0.5*Y - 0.6 > 0

fig = plt.figure(dpi=200)

heatmap = plt.pcolormesh(X, Y, Z, shading="auto", cmap=tc.tol_cmap("sunset"))
plt.colorbar(heatmap)
plt.show()
```

On confirmeâ€¯: Ã§a marcheâ€¯!


Ã‡a marche aussi trÃ¨s bien pour $\operatorname{OU}$ et $\operatorname{NON}$

## ğŸ™…ğŸ» Exo ğŸ™…ğŸ»

DÃ©terminer la structure et les poids Ã  utiliser pour implÃ©menter une porte OU et une porte NON avec
des perceptrons simples.

<!-- ```python
or_weights = np.array([-0.5, 1, 1])
print("x\ty\tx OU y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = perceptron([x_i, y_i], or_weights).astype(int)
        print(f"{x_i}\t{y_i}\t{out}")
```

```python
not_weights = np.array([1, -1])
print("x\tNON x")
for x_i in [0, 1]:
    out = perceptron([x_i], not_weights).astype(int)
    print(f"{x_i}\t{out}")
``` -->

## Algorithme du perceptron

### ğŸ² Exo ğŸ²

<small>TirÃ© du [cours de FranÃ§ois
Denis](https://pageperso.lis-lab.fr/~francois.denis/IAAM1/TP3_Perceptron.pdf)</small>

La fonction `random_nice_dataset` du script [`perceptron_data.py`](perceptron_data.py) permet de
gÃ©nÃ©rer un jeu de donnÃ©es alÃ©atoire linÃ©airement sÃ©parable en deux dimensions.

```python
import perceptron_data

perceptron_data.random_nice_dataset(4)
```

1\. Ã€ l'aide de cette fonction, gÃ©nÃ©rer un jeu de donnÃ©es d'entraÃ®nement et un jeu de donnÃ©es de
test (par exemple de tailles respectivement 128 et 64).

2\. Appliquer l'algorithme du perceptron sur pour apprendre les donnÃ©es de d'entraÃ®nement (sans
utiliser de terme de biais) et tester les performances du classifieur ainsi appris sur les donnÃ©es
de test.

3\. ReprÃ©senter (par exemple avec `matplotlib.pyplot`) les donnÃ©es de test (en utilisant des
couleurs diffÃ©rentes pour les deux classes) ainsi que la frontiÃ¨re du classifieur.

### â• Exo â•

Le jeu de donnÃ©es `perceptron_data.bias` reprÃ©sente un problÃ¨me de classification Ã  une dimension,
mais pour lequel un terme de biais est nÃ©cessaire.

1\. Appliquer votre implÃ©mentation prÃ©cÃ©dente de l'algorithme du perceptron (pour un nombre grand
mais fixÃ© d'epochs) pour constater la non-convergence (par exemple en affichant les poids et
l'erreur de classification moyenne Ã  chaque Ã©tape).

2\. Modifier votre implÃ©mentation pour introduire un terme de biais et montrer que dans ce cas
l'apprentissage converge.

## Perceptron multi-classe

### ğŸŒ· Exo ğŸŒ·

Appliquer l'algorithme du perceptron multiclasse au jeu de donnÃ©es `perceptron_data.iris` et tester
ses performances avec une validation croisÃ©e Ã  8 plis.
