---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.14.1
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- LTeX: language=fr -->
<!-- #region slideshow={"slide_type": "slide"} -->
Le perceptron simpleâ€¯: solutions
===============================

**LoÃ¯c Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

<!-- #endregion -->

```python
from IPython.display import display, Markdown
```

```python
import numpy as np
import matplotlib.pyplot as plt
```

## Principe et historique

### ğŸ™…ğŸ» Exo ğŸ™…ğŸ»

> DÃ©terminer la structure et les poids Ã  utiliser pour implÃ©menter une porte OU et une porte NON
> avec des perceptrons simples.

```python
or_weights = np.array([-0.5, 1, 1])
print("x\ty\tx OU y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = perceptron([x_i, y_i], or_weights).astype(int)
        print(f"{x_i}\t{y_i}\t{out}")
```

```python
not_weights = np.array([1, -1])
print("x\tNON x")
for x_i in [0, 1]:
    out = perceptron([x_i], not_weights).astype(int)
    print(f"{x_i}\t{out}")
```

## Algorithme du perceptron

L'algorithme du perceptron de Rosenblatt permet de trouver des poids pour lesquels le perceptron
simple partitionne de faÃ§on exacte un jeu de donnÃ©es avec un Ã©tiquetage linÃ©airement sÃ©parable.

On va supposer ici pour simplifier les notations qu'on utilise comme classe $-1$ et $1$ au lieu de
$0$ et $1$ et on considÃ¨re un taux d'apprentissage $Î±>0$.

Alors l'algorithme prend la forme suivanteâ€¯:

- Initialiser le vecteur de poids $W$ Ã  des valeurs arbitraires.
- Tant qu'il reste des points $X$ mal classifiÃ©s.

  - Pour chaque couple $(X, y) \in \mathcal{D}$â€¯:

    - Calculer $z = \langle W | X \rangle$.
    - Si $yÃ—z â‰¤ 0$:
      - $Wâ†W+Î±Ã—yÃ—X$

Notez queâ€¯:

- La condition $yÃ—z â‰¤ 0$ est une faÃ§on compressÃ©e de dire â€œsi $y$ et $z$ sont de mÃªme signeâ€ et donc
  â€œsi $\hat{y}= yâ€.
- La mise Ã  jour de $W$ va tirer $z$ dans la direction de $y$â€¯: calculer $\langle W + Î±yX | X
  \rangle$ pour s'en convaincre.
- On peut compresser la condition et la mise Ã  jour en une seule ligneâ€¯: $Wâ†W+Î±(y-\hat{y})X$.

Sous rÃ©serve que le jeu de donnÃ©es soit effectivement linÃ©airement sÃ©parable, l'algorithme termine
toujours (et on peut mÃªme estimer sa vitesse de convergence), un rÃ©sultat parfois appelÃ© *thÃ©orÃ¨me
de convergence de Novikov*.

### ğŸ² Exo ğŸ²

<small>TirÃ© du [cours de FranÃ§ois
Denis](https://pageperso.lis-lab.fr/~francois.denis/IAAM1/TP3_Perceptron.pdf)</small>

La fonction `random_nice_dataset` du script [`perceptron_data.py`](perceptron_data.py) permet de
gÃ©nÃ©rer un jeu de donnÃ©es alÃ©atoire linÃ©airement sÃ©parable en deux dimensions.

```python
import perceptron_data

perceptron_data.random_nice_dataset(4)
```

1\. Ã€ l'aide de cette fonction, gÃ©nÃ©rer un jeu de donnÃ©es d'entraÃ®nement et un jeu de donnÃ©es de
test (par exemple de tailles respectivement 128 et 64).

2\. Appliquer l'algorithme du perceptron sur pour apprendre les donnÃ©es de d'entraÃ®nement (sans
utiliser de terme de biais) et tester les performances du classifieur ainsi appris sur les donnÃ©es
de test.

3\. ReprÃ©senter (par exemple avec `matplotlib.pyplot`) les donnÃ©es de test (en utilisant des
couleurs diffÃ©rentes pour les deux classes) ainsi que la frontiÃ¨re du classifieur.

### â• Exo â•

Le jeu de donnÃ©es `perceptron_data.bias` reprÃ©sente un problÃ¨me de classification Ã  une dimension,
mais pour lequel un terme de biais est nÃ©cessaire.

1\. Appliquer votre implÃ©mentation prÃ©cÃ©dente de l'algorithme du perceptron (pour un nombre grand,
mais fixÃ©) d'epochs pour constater la non-convergence (par exemple en affichant les poids et
l'erreur de classification moyenne Ã  chaque Ã©tape).

2\. Modifier votre implÃ©mentation pour introduire un terme de biais et montrer que dans ce cas
l'apprentissage converge.

## Perceptron multi-classe

Le cas d'un problÃ¨me de classification Ã  $n$ classe se traite en prÃ©disant un score par classe avec
une fonction linÃ©aire par classe et en affectant la classe pour laquelle le score est maximal.
Formellement on dispose donc d'un $n$-uplet de poids $W_1, â€¦, W_n$ et on a pour un exemple $X$â€¯:

$$\begin{align}
    z_1 &= \langle W_1 | X \rangle\\
        &â‹®\\
    z_n &= \langle W_n | X \rangle
    \hat{y} &= \argmax_k z_k
\end{align}$$

Moralement on peut y penser comme avoir $n$ perceptrons, un par classe.

L'algorithme d'apprentissage du perceptron simple simple s'adapte simplementâ€¯: pour les exemples mal
classifiÃ©s on ajuste les $W_k$ de faÃ§on Ã  ce que le score $z_y$ de la classe correcte augmente et Ã 
ce que le score de la classe prÃ©dite diminue. En pseudo-codeâ€¯:

- Tant qu'on est pas satisfaitâ‹…e:
  - Pour chaque $(X, y)âˆˆ\mathcal{D}$:
    - Pour chaque $kâˆˆâŸ¦1, nâŸ§$:
      - Calculer $z_k=\langle W_k | X \rangle$
    - DÃ©terminer $\hat{y}=\argmax_k z_k$
    - Si $\hat{y}\neq y$:
      - $W_yâ†W_y+Î±X$
      - $W_{\hat{y}}â†W_{\hat{y}}-Î±X$

Le critÃ¨re de satisfaction peut Ãªtre comme prÃ©cÃ©demment â€œjusqu'Ã  ce qu'il n'y ait plus d'erreurâ€,
mais comme prÃ©cÃ©demment ce n'est atteignable qu'avec des conditions contraignantes sur les donnÃ©es.
Un critÃ¨re d'arrÃªt plus rÃ©aliste peut Ãªtre un nombre maximal d'Ã©tapes ou l'arrÃªt de l'amÃ©lioration
des performances.

**Note**â€¯: calculer les $n$ produits scalaires $\langle W_k | X \rangle$ revient Ã  multiplier $X$ Ã 
gauche par la matrice dont les colonnes sont les $W_k$. Autrement dit si on note $W_k = (w_{â„“, k})$
et qu'on pose

$$
W =
    \begin{pmatrix}
        w_{1,1} & \ldots & w_{1,k}\\
        \vdots  & \ddots & \vdots\\
        w_{1,n} & \ldots & w_{n,k}\\
    \end{pmatrix}
$$

Alors on a

$$
Z = \begin{pmatrix}z_1\\\vdots\\z_n\end{pmatrix} = WÃ—X
$$

Le calcul de ce produit matriciel Ã©tant beaucoup plus rapide sur machine que l'Ã©criture d'une
boucle, il est fortement recommandÃ© de l'utiliser, l'algorithme devenant alorsâ€¯:

- Tant qu'on est pas satisfaitâ‹…e:
  - Pour chaque $(X, y)âˆˆ\mathcal{D}$:
    - Calculer $Z=WÃ—X$
    - DÃ©terminer $\hat{y}=\argmax_k z_k$
    - Si $\hat{y}\neq y$:
      - $W_yâ†W_y+Î±X$
      - $W_{\hat{y}}â†W_{\hat{y}}-Î±X$

### ğŸŒ· Exo ğŸŒ·

Appliquer l'algorithme du perceptron multiclasse au jeu de donnÃ©es `perceptron_data.iris` et tester
ses performances avec une validation croisÃ©e Ã  8 plis.
