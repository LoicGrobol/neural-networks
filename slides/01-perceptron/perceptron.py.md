---
jupyter:
  jupytext:
    custom_cell_magics: kql
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.18.1
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- LTeX: language=fr -->
<!-- #region slideshow={"slide_type": "slide"} -->
Cours 01: Le perceptron simple
==============================

**LoÃ¯c Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

<!-- #endregion -->

```python
from IPython.display import display, Markdown
```

```python
import numpy as np
import matplotlib.pyplot as plt
```

## Principe et historique

[![SchÃ©ma d'un neurone avec des lÃ©gendes pour les organelles et les connexions importantes pour la
communication entre
neurones.](https://upload.wikimedia.org/wikipedia/commons/1/10/Blausen_0657_MultipolarNeuron.png)](https://commons.wikimedia.org/w/index.php?curid=28761830)

Un modÃ¨le de neurone biologique (plutÃ´t sensoriel)â€¯: une unitÃ© qui reÃ§oit plusieurs entrÃ©es $x_j$
scalaires (des nombres quoi), en calcule une somme pondÃ©rÃ©e $z$ (avec des poids $w_j$ prÃ©dÃ©finis) et
renvoie une sortie binaire $y$ ($1$ si $z$ est positif, $0$ sinon).


Autrement dit

$$
\begin{align}
z &= \sum_j w_jx_j = w_1 x_1 + w_2 x_2 + â€¦ + w_n x_n\\
\hat{y} &=
    \begin{cases}
        1 & \text{si $z > 0$}\\
        0 & \text{sinon}
    \end{cases}
\end{align}
$$

FormulÃ© cÃ©lÃ¨brement par McCulloch et Pitts (1943) avec des notations diffÃ©rentes.

ImplÃ©mentÃ© comme une machine, le perceptron Mark I, par Rosenblatt (1958)â€¯:

[![Une photographie en noir et blanc d'une machine ressemblant Ã  une grande armoire pleine de fils
Ã©lectriques](https://upload.wikimedia.org/wikipedia/en/5/52/Mark_I_perceptron.jpeg)](https://en.wikipedia.org/wiki/File:Mark_I_perceptron.jpeg)

**Attention** selon les auteurices, le cas $z=0$ est traitÃ© diffÃ©remment, pour *Speech and Language
Processing*, par exemple, on renvoie $0$ dans ce cas, c'est donc la convention qu'on suivra, mais
vÃ©rifiez Ã  chaque fois.

**Note**: si on note $W$ le vecteur dont les coordonnÃ©es sont les $w_j$ et $X$ celui dont les
coordonnÃ©s sont les $x_j$, $z$ est le **produit scalaire** de $W$ et $X$, notÃ© $\langle W | X
\rangle$.


On peut ajouter un terme de *biais* en fixant $x_0=1$ et $w_0=b$, ce qui donne

$$
\begin{equation}
    z = \sum_{j=0}^n w_jx_j = \sum_{j=1}^n w_jx_j + b
\end{equation}
$$

Ou schÃ©matiquement


![](figures/perceptron/perceptron.svg)


Ou avec du code

```python
def perceptron(inpt, weight):
    z = weight[0] # On commence par le terme de biais
    for i in range(len(inpt)):  # Attention, en Python les indices commencent Ã  0
        z += inpt[i]*weight[i]
    if z > 0:
        y = 1
    else:
        y = 0
    return y

perceptron([-2.0, -1.0], [-0.5, 0.5])
```

```python
def perceptron(inpt, weight):
    z = weight[0]
    for x, w in zip(inpt, weight[1:]):
        z += w*x
    if z > 0:
        y = 1
    else:
        y = 0
    return y

perceptron([-2.0, -1.0], [1.0, -0.5, 0.5])
```

```python
def perceptron(inpt, weight):
    """Calcule la sortie du perceptron dont les poids sont `weights` pour l'entrÃ©e `inpt`

    EntrÃ©esâ€¯:

    - `inpt` un tableau numpy de dimension $n$
    - `weights` un tableau numpy de dimention $n+1$

    Sortie: un tableau numpy de type boolÃ©en et de dimensions $0$
    """
    return np.greater(np.inner(weight, np.concatenate([[1.0], inpt])), 0.0).astype(np.int64)
```



**Est-ce que Ã§a vous rappelle quelque choseâ€¯?**


ğŸ¤”


C'est un **classifieur linÃ©aire** dont on a dÃ©jÃ  parlÃ© dans le cours prÃ©cÃ©dent.


Les ambitions initiales Ã©taient grandes

> *the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see,
> write, reproduce itself and be conscious of its existence.*  
> New York Times, rapportÃ© par Olazaran (1996)


C'est par exemple assez facile de construire un qui rÃ©alise l'opÃ©ration logique Ã©lÃ©mentaire
$\operatorname{ET}$Â :

```python
and_weights = np.array([-0.6, 0.5, 0.5])
print("x\ty\tx ET y")
for x in [0, 1]:
    for y in [0, 1]:
        out = perceptron([x, y], and_weights)
        print(f"{x}\t{y}\t{out}")
```

Ã‡a marche bien parce que c'est un problÃ¨me **linÃ©airement sÃ©parable**â€¯: si on reprÃ©sente $x$ et $y$
dans le plan, on peut tracer une droite qui sÃ©pare la partie oÃ¹ $x\,\operatorname{ET}\,y$ vaut $1$ et
la partie oÃ¹ Ã§a vaut $0$â€¯:

```python
x = np.array([0, 1])
y = np.array([0, 1])
X, Y = np.meshgrid(x, y)
Z = np.logical_and(X, Y)

fig = plt.figure(dpi=200)

heatmap = plt.scatter(X, Y, c=Z)
plt.colorbar(heatmap)
plt.show()
```

Ici voilÃ  les valeurs que renvoie notre neuroneâ€¯:

```python
x = np.linspace(0, 1, 1000)
y = np.linspace(0, 1, 1000)
X, Y = np.meshgrid(x, y)
Z = 0.5*X + 0.5*Y - 0.6 > 0

fig = plt.figure(dpi=200)

heatmap = plt.pcolormesh(X, Y, Z, shading="auto")
plt.colorbar(heatmap)
plt.show()
```

On confirmeâ€¯: Ã§a marcheâ€¯!


Ã‡a marche aussi trÃ¨s bien pour $\operatorname{OU}$ et $\operatorname{NON}$

### ğŸ™…ğŸ» Exo ğŸ™…ğŸ»

DÃ©terminer la structure et les poids Ã  utiliser pour implÃ©menter une porte OU et une porte NON avec
des perceptrons simples.

<!-- ```python
or_weights = np.array([-0.5, 1, 1])
print("x\ty\tx OU y")
for x_i in [0, 1]:
    for y_i in [0, 1]:
        out = perceptron([x_i, y_i], or_weights).astype(int)
        print(f"{x_i}\t{y_i}\t{out}")
```

```python
not_weights = np.array([1, -1])
print("x\tNON x")
for x_i in [0, 1]:
    out = perceptron([x_i], not_weights).astype(int)
    print(f"{x_i}\t{out}")
``` -->

## Algorithme du perceptron

L'algorithme du perceptron de Rosenblatt permet de trouver des poids pour lesquels le perceptron
simple partitionne de faÃ§on exacte un jeu de donnÃ©es avec un Ã©tiquetage linÃ©airement sÃ©parable.

On considÃ¨re un nombre $Î±>0$, le *taux d'apprentissage*.

L'algorithme prend la forme suivanteâ€¯:

- Initialiser le vecteur de poids $W$ Ã  des valeurs arbitraires.
- Tant qu'il reste des points $X$ mal classifiÃ©s.

  - Pour chaque couple $(X, y) \in \mathcal{D}$â€¯:

    - Si $y = 1$, on pose $y'=1$, et si $y=0$, $y'=-1$
    - Calculer $z = \langle W | X \rangle$.
    - Si $y'Ã—z â‰¤ 0$:
      - $Wâ†W+Î±Ã—y'Ã—X$

Notez queâ€¯:

- La condition $y'Ã—z â‰¤ 0$ est une faÃ§on compressÃ©e de dire â€œsi $y'$ et $z$ sont de signes opposÃ©sâ€ et donc
  â€œsi $\hat{y} â‰  y$â€.
- La mise Ã  jour de $W$ rÃ©duit son Ã©cart angulaire avec $X$ si $y=1$ et l'augmente de $X$ si $y=0$.
  Pour s'neconvaincre, faire un dessin, puis calculer $\langle W+ay'X | X \rangle$.
- On peut compresser la condition et la mise Ã  jour en une seule ligneâ€¯: $Wâ†W+Î±(y-\hat{y})X$.
- Pour Ã©conomiser un test, on peut simplement Ã©crire $y'=2y-1$.

Sous rÃ©serve que le jeu de donnÃ©es soit effectivement linÃ©airement sÃ©parable, l'algorithme termine
toujours (et on peut mÃªme estimer sa vitesse de convergence), un rÃ©sultat parfois appelÃ© *thÃ©orÃ¨me
de convergence de Novikov*.

### ğŸ² Exo ğŸ²

<small>TirÃ© du [cours de FranÃ§ois
Denis](https://pageperso.lis-lab.fr/~francois.denis/IAAM1/TP3_Perceptron.pdf)</small>

La fonction `random_nice_dataset` du script [`perceptron_data.py`](perceptron_data.py) permet de
gÃ©nÃ©rer un jeu de donnÃ©es alÃ©atoire linÃ©airement sÃ©parable en deux dimensions.

```python
import perceptron_data

perceptron_data.random_nice_dataset(4)
```

1\. Ã€ l'aide de cette fonction, gÃ©nÃ©rer un jeu de donnÃ©es d'entraÃ®nement et un jeu de donnÃ©es de
test (par exemple de tailles respectivement 128 et 64).

2\. Appliquer l'algorithme du perceptron sur les donnÃ©es que vous venez de gÃ©nÃ©rer pour apprendre
les donnÃ©es d'entraÃ®nement (sans utiliser de terme de biais) et tester les performances du
classifieur ainsi appris sur les donnÃ©es de test.

3\. ReprÃ©senter (par exemple avec `matplotlib.pyplot`) les donnÃ©es de test (en utilisant des
couleurs diffÃ©rentes pour les deux classes) ainsi que la frontiÃ¨re du classifieur.

## Perceptron multi-classe

Le cas d'un problÃ¨me de classification Ã  $n$ classes se traite en prÃ©disant un score par classe avec
une fonction linÃ©aire par classe et en affectant la classe pour laquelle le score est maximal.
Formellement, on dispose donc d'un $n$-uplet de poids $W_1, â€¦, W_n$ et on a pour un exemple $X$â€¯:

$$
\begin{align}
    z_1 &= \langle W_1 | X \rangle\\
        &â‹®\\
    z_n &= \langle W_n | X \rangle
    \hat{y} &= \argmax_k z_k
\end{align}
$$

Moralement, on peut y penser comme avoir $n$ perceptrons, un par classe.

L'algorithme d'apprentissage du perceptron simple s'adapte simplementâ€¯: pour les exemples mal
classifiÃ©sâ€¯: on ajuste les $W_k$ de faÃ§on Ã  ce que le score $z_y$ de la classe correcte augmente et
Ã  ce que le score de la classe prÃ©dite diminue (puisqu'elle est incorrecte). En pseudo-codeâ€¯:

- Tant qu'on est pas satisfaitâ‹…e:
  - Pour chaque $(X, y)âˆˆ\mathcal{D}$:
    - Pour chaque $kâˆˆâŸ¦1, nâŸ§$:
      - Calculer $z_k=\langle W_k | X \rangle$
    - DÃ©terminer $\hat{y}=\argmax_k z_k$
    - Si $\hat{y}\neq y$:
      - $W_yâ†W_y+Î±X$
      - $W_{\hat{y}}â†W_{\hat{y}}-Î±X$

Le critÃ¨re de satisfaction peut Ãªtre comme prÃ©cÃ©demment â€œjusqu'Ã  ce qu'il n'y ait plus d'erreurâ€,
mais aussi comme prÃ©cÃ©demment ce n'est atteignable qu'avec des conditions contraignantes sur les
donnÃ©es. Un critÃ¨re d'arrÃªt plus rÃ©aliste peut Ãªtre un nombre maximal d'Ã©tapes ou l'arrÃªt de
l'amÃ©lioration des performances.

**Note**â€¯: calculer les $n$ produits scalaires $\langle W_k | X \rangle$ revient Ã  multiplier $X$ Ã 
gauche par la matrice dont les colonnes sont les $W_k$. Autrement dit si on note $W_k = (w_{â„“, k})$
et qu'on pose

$$
W =
    \begin{pmatrix}
        w_{1,1} & \ldots & w_{1,k}\\
        \vdots  & \ddots & \vdots\\
        w_{1,n} & \ldots & w_{n,k}\\
    \end{pmatrix}
$$

Alors on a

$$
Z = \begin{pmatrix}z_1\\\vdots\\z_n\end{pmatrix} = WÃ—X
$$

Le calcul de ce produit matriciel Ã©tant beaucoup plus rapide sur machine que l'Ã©criture d'une
boucle, il est fortement recommandÃ© de l'utiliser, l'algorithme devenant alorsâ€¯:

- Tant qu'on est pas satisfaitâ‹…e:
  - Pour chaque $(X, y)âˆˆ\mathcal{D}$:
    - Calculer $Z=WÃ—X$
    - DÃ©terminer $\hat{y}=\argmax_k z_k$
    - Si $\hat{y}\neq y$:
      - $W_yâ†W_y+Î±X$
      - $W_{\hat{y}}â†W_{\hat{y}}-Î±X$

### ğŸŒ· Exo ğŸŒ·

Appliquer l'algorithme du perceptron multiclasse au jeu de donnÃ©es `perceptron_data.iris` et tester
ses performances avec une validation croisÃ©e Ã  8 plis.
