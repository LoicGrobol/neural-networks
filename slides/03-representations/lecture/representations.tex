% LTeX: language=fr
% Copyright © 2023, Loïc Grobol <loic.grobol@gmail.com>
% This document is available under the terms of the Creative Commons Attribution 4.0 International License (CC BY 4.0) (https://creativecommons.org/licenses/by/4.0/)

% Settings
\newcommand\myname{L. Grobol}
\newcommand\mylab{MoDyCo, Université Paris Nanterre}
\newcommand\pdftitle{Apprentissage Automatique : représentation des données}
\newcommand\mymail{lgrobol@parisnanterre.fr}
\newcommand\titlepagetitle{\pdftitle}
\newcommand\eventname{M2 Plurital}
\newcommand\eventvenue{Nanterre, France}
\newcommand\eventdate{2024-12-04}

\documentclass[
	xcolor={svgnames},
	aspectratio=169,
	french,
]{beamer}
% Colour palettes from [Paul Tol's technical note](https://personal.sron.nl/~pault/data/colourschemes.pdf) v3.1
% Bright scheme
\definecolor{sronbrightblue}{RGB}{68, 119, 170}
\definecolor{sronbrightcyan}{RGB}{102, 204, 238}
\definecolor{sronbrightgreen}{RGB}{34, 136, 51}
\definecolor{sronbrightyellow}{RGB}{204, 187, 68}
\definecolor{sronbrightred}{RGB}{238, 102, 119}
\definecolor{sronbrightpurple}{RGB}{170, 51, 119}
\definecolor{sronbrightgrey}{RGB}{187, 187, 187}

% Divergent colour scheme (fig 19 in Paul Tol's note)
\definecolor{ptolD01}{RGB}{232,236,251}
\definecolor{ptolD02}{RGB}{217,204,227}
\definecolor{ptolD03}{RGB}{209,187,215}
\definecolor{ptolD04}{RGB}{202,172,203}
\definecolor{ptolD05}{RGB}{186,141,180}
\definecolor{ptolD06}{RGB}{174,118,163}
\definecolor{ptolD07}{RGB}{170,111,158}
\definecolor{ptolD08}{RGB}{153,79,136}
\definecolor{ptolD09}{RGB}{136,46,114}
\definecolor{ptolD10}{RGB}{25,101,176}
\definecolor{ptolD11}{RGB}{67,125,191}
\definecolor{ptolD12}{RGB}{82,137,199}
\definecolor{ptolD13}{RGB}{97,149,207}
\definecolor{ptolD14}{RGB}{123,175,222}
\definecolor{ptolD15}{RGB}{78,178,101}
\definecolor{ptolD16}{RGB}{144,201,135}
\definecolor{ptolD17}{RGB}{202,224,171}
\definecolor{ptolD18}{RGB}{247,240,86}
\definecolor{ptolD20}{RGB}{246,193,65}
\definecolor{ptolD22}{RGB}{241,147,45}
\definecolor{ptolD24}{RGB}{232,96,28}
\definecolor{ptolD25}{RGB}{230,85,24}
\definecolor{ptolD26}{RGB}{220,5,12}
\definecolor{ptolD27}{RGB}{165,23,14}
\definecolor{ptolD28}{RGB}{114,25,14}
\definecolor{ptolD29}{RGB}{66,21,10}

\definecolor{sronmutedindigo}{RGB}{51,34,136}

% And my favourite purple
\definecolor{myfavouritepurple}{RGB}{113, 10, 186}
\definecolor{electricindigo}{RGB}{111, 0, 255}
\definecolor{neonpink}{RGB}{255, 68, 204}


\usetheme[
	sectionpage=progressbar,
	subsectionpage=progressbar,
	progressbar=frametitle,
]{metropolis}
	\colorlet{accent}{neonpink}
	\setbeamercolor{frametitle}{
		use=normal text,
		bg=normal text.bg,
		fg=accent,
	}
	\setbeamercolor{alerted text}{fg=accent}
	\setbeamercolor{progress bar}{fg=accent}
	\makeatletter
		\setlength{\metropolis@progressinheadfoot@linewidth}{0.5pt}
	\makeatother

% Left-align description lists
\defbeamertemplate{description item}{align left}{\insertdescriptionitem\hfill}
\setbeamertemplate{description item}[align left]

% Use non-standard fonts
\usepackage{fontspec}
	\usefonttheme{professionalfonts}

	\directlua{
		luaotfload.add_fallback(
			"myfallback",
			{
				"NotoColorEmoji:mode=harf;",
				"NotoSans:mode=harf;",
				"DejaVuSans:mode=harf;",
			}
		)
	}
	
	\setsansfont{Fira Sans}[
		BoldFont={* Semibold},
		RawFeature={fallback=myfallback;multiscript=auto;},
	]
	\setmonofont[Scale=0.9]{Fira Mono}
	\newfontfamily\fallbackfont{Deja Vu Sans}
	\newfontfamily\emojifont{Noto Color Emoji}[Renderer=HarfBuzz]
	\frenchspacing

% Fix missing glyphs in Fira by delegating to polyglossia/babel
\usepackage{newunicodechar}
	\newunicodechar{ }{~}   % U+202F NARROW NO-BREAK SPACE
	\newunicodechar{ }{ }   % U+2009 THIN SPACE

% Notes on left screen
% \usepackage{pgfpages}
% \setbeameroption{show notes on second screen=left}

\usepackage{polyglossia}
	\setmainlanguage{french}
	\setotherlanguage[variant=british]{english}
	\setotherlanguage{breton}
	\NewDocumentCommand\borrowing{ m m }{
		\textlang{#1}{\textit}	
	}
\usepackage{amsfonts,amssymb}
\usepackage{amsmath,amsthm}
\usepackage{mathtools}	% AMS Maths service pack
	\newtagform{brackets}{[}{]}	% Pour des lignes d'équation numérotées entre crochets
	\mathtoolsset{showonlyrefs, showmanualtags, mathic}	% affiche les tags manuels (\tag et \tag*) et corrige le kerning des maths inline dans un bloc italique voir la doc de mathtools
	\usetagform{brackets}	% Utilise le style de tags défini plus haut
\usepackage{lualatex-math}

\usepackage[math-style=french]{unicode-math}
	\setmathfont[Scale=1.3]{Libertinus Math}
\usepackage{newunicodechar}
	\newunicodechar{√}{\sqrt}
\usepackage{mleftright}

% Fix incompatibility with unicode-math
\let\UnicodeMathMathBfSfIt\mathbfsfit
\let\mathbfsfit\relax
\usepackage{mismath}
\let\mathbfsfit\UnicodeMathMathBfSfIt

\usepackage{covington}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{siunitx}
	\sisetup{
		detect-all,
		group-separator=\text{\,},
	}
	\DeclareSIUnit{\quantity}{\relax}
	\DeclareSIUnit{\words}{words}
	\DeclareSIUnit{\sentences}{sentences}
	% Needed for italics and bold numbers in siunitx S-aligned columns
	\robustify\itshape
	\robustify\bfseries
\usepackage{multicol}
\usepackage{ccicons}
\usepackage{bookmark}
\usepackage{caption}
	\captionsetup{skip=1ex, labelformat=empty}
\usepackage{lua-ul}
% \usepackage{minted}
% 	\usemintedstyle{lovelace}
% 	\setminted{autogobble, fontsize=\normalsize, tabsize=4}
% 	\setmintedinline{fontsize=auto}
% 	\RenewDocumentCommand\listingscaption{}{Example} 

\usepackage[
	english=american,
	french=guillemets,
	autostyle=true,
]{csquotes}
	\renewcommand{\mkbegdispquote}[2]{\itshape\let\emph\textbf}
	% Like `\foreignquote` but use the outside language's quotes not the inside's
	\NewDocumentCommand\quoteforeign{m m}{\enquote{\textlang{#1}{\textit{#2}}}}

\usepackage{tikz}
	\NewDocumentCommand\textnode{O{}mm}{
		\tikz[remember picture, baseline=(#2.base), inner sep=0pt]{\node[#1] (#2) {#3};}
	}
	\NewDocumentCommand\mathnode{O{}mm}{
		\tikz[remember picture, baseline=(#2.base), inner sep=0pt]{\node[#1] (#2) {\(\displaystyle #3\)};}
	}
	% Beamer utilities
	\tikzset{
		alt/.code args={<#1>#2#3}{%
		  \alt<#1>{\pgfkeysalso{#2}}{\pgfkeysalso{#3}} % \pgfkeysalso doesn't change the path
		},
		invisible/.style={opacity=0},
		visible on/.style={alt={<#1>{}{invisible}}},
		accent on/.style={alt={<#1>{draw=accent, text=accent, thick}{draw}}},
	}
	% Misc utilities
	\tikzset{
		true scale/.style={scale=#1, every node/.style={transform shape}},
	}
	% Custom styles
	\tikzset{
		>=stealth,
		hair lines/.style={line width = 0.05pt, lightgray},
		accent on/.style={alt={<#1>{draw=accent, text=accent, thick}{draw}}},
		true scale/.style={scale=#1, every node/.style={transform shape}},
	}	

	\usetikzlibrary{tikzmark}
	\usetikzlibrary{matrix, chains, graphs, graphdrawing}
	\usetikzlibrary{shapes, shapes.geometric}
	\usetikzlibrary{decorations.pathreplacing}
	\usetikzlibrary{decorations.pathmorphing}
	\usetikzlibrary{positioning, calc, intersections}
	\usetikzlibrary{fit}

% % Highlight formulas
% \usepackage[beamer, markings]{hf-tikz}

% \usepackage{forest}
% 	\useforestlibrary{linguistics}

% \usepackage{tikz-dependency}

% Plots
\usepackage{pgfplots}
	\pgfplotsset{compat=1.18}
	% Due to pgfplots meddling with pgfkeys, we have to redefine alt here.
	\pgfplotsset{
		alt/.code args={<#1>#2#3}{%
		\alt<#1>{\pgfkeysalso{#2}}{\pgfkeysalso{#3}} % \pgfkeysalso doesn't change the path
		},
	}
	\pgfplotsset{compat=1.18}
	\pgfplotsset{colormap={SRON}{rgb255=(61,82,161) rgb255=(255,250,210) rgb255=(174,28,62)}} % chktex 36

% \usepackage{robust-externalize}

	% \robExtConfigure{
	% 	add to preset={tikz}{
	% 	% we load some packages that will be loaded by figures based on the tikz preset
	% 	add to preamble={\usepackage{pifont}}
	% 	}
	% }

\usepackage[
	block=ragged,
	dashed=false,
	doi=false,
	isbn=false,
	maxbibnames=6,
	maxcitenames=2,
	minbibnames=1,
	mincitenames=1,
	uniquelist=false,
	useprefix=true,
	style=authoryear,
]{biblatex}
	% No small caps in french bib
	\DefineBibliographyExtras{french}{\restorecommand\mkbibnamefamily}
	\AtEveryBibitem{
		\ifentrytype{online}
		{} {
			\iffieldequalstr{howpublished}{online}
			{
				\clearfield{howpublished}
			} {
				\clearfield{urlyear}\clearfield{urlmonth}\clearfield{urlday}
			}
		}
	}
	% Fix bug with \insertbiblabel in author-date, see https://tex.stackexchange.com/questions/585635/beamer-biblatex-authoryear-causes-problem-with-insertbiblabel and https://github.com/josephwright/beamer/blob/865a19d4ec64f4c8e4935c19e162b8f4fd5aa190/base/beamerbaselocalstructure.sty#L501
	\let\insertbiblabel\relax
	\addbibresource{biblio.bib}
 
% Compact bibliography style
\setbeamertemplate{bibliography item}[text]

\AtEveryBibitem{
	\clearfield{series}
	\clearfield{pages}
	\clearname{editor}
}
\renewcommand*{\bibfont}{\tiny}

\usepackage{hyperxmp}	% XMP metadata

\usepackage[type={CC},modifier={by},version={4.0}]{doclicense}

% \usepackage{todonotes}
% 	\let\todox\todo
% 	\renewcommand\todo[1]{\todox[inline]{#1}}

\title{\titlepagetitle}
\author{\textbf{\myname}~(\mylab)}
\institute{}
\date{\eventname\\\eventvenue, \eventdate}

\titlegraphic{\ccby}

% Commands spécifiques
\NewDocumentCommand\shorturl{ O{https} O{://} m }{%
	\href{#1#2#3}{\nolinkurl{#3}}%
}

\DeclarePairedDelimiterX\compset[2]{\lbrace}{\rbrace}{#1\,\delimsize|\,#2}
\DeclarePairedDelimiterX\innprod[2]{\langle}{\rangle}{#1\,\delimsize|\,#2}

% Easy column vectors \vcord{a,b,c} ou \vcord[;]{a;b;c}
% Here be black magic
\ExplSyntaxOn % chktex 1
	\NewDocumentCommand{\vcord}{O{,}m}{\vector_main:nnnn{p}{\\}{#1}{#2}}
	\NewDocumentCommand{\tvcord}{O{,}m}{\vector_main:nnnn{psmall}{\\}{#1}{#2}}
	\seq_new:N\l__vector_arg_seq
	\cs_new_protected:Npn\vector_main:nnnn #1 #2 #3 #4{
		\seq_set_split:Nnn\l__vector_arg_seq{#3}{#4}
		\begin{#1matrix}
			\seq_use:Nnnn\l__vector_arg_seq{#2}{#2}{#2}
		\end{#1matrix}
	}
\ExplSyntaxOff % chktex 1

\NewDocumentCommand\itpause{}{%
	\addtocounter{beamerpauses}{-1}%
	\pause%
}

\NewDocumentCommand\graphdot{O{fill=ptolD10} r() O{0.5ex}}{\path[#1] (#2) circle (#3)}


% ██████   ██████   ██████ ██    ██ ███    ███ ███████ ███    ██ ████████
% ██   ██ ██    ██ ██      ██    ██ ████  ████ ██      ████   ██    ██
% ██   ██ ██    ██ ██      ██    ██ ██ ████ ██ █████   ██ ██  ██    ██
% ██   ██ ██    ██ ██      ██    ██ ██  ██  ██ ██      ██  ██ ██    ██
% ██████   ██████   ██████  ██████  ██      ██ ███████ ██   ████    ██

\begin{document}
\pdfbookmark[2]{Title}{title}

\begin{frame}[plain]
	\titlepage
\end{frame}

\begin{frame}[label=kevrin, standout]
	\quoteforeign{breton}{kevrin} : nom ou verbe ?
\end{frame}

\begin{frame}<1>[label=lexicon, plain]
	% See <https://tex.stackexchange.com/a/154322> for why the mboxes
	\begin{columns}
		\column{0.49\textwidth}
			\begin{description}
				\item[\textlang{breton}{sardin}] \mbox{}\visible<2->{sardine}
				\item[\textlang{breton}{gwez}] \mbox{}\visible<2->{arbres}
				\item[\textlang{breton}{reizh}] \mbox{}\visible<2->{genre}
				\item[\textlang{breton}{kentel}] \mbox{}\visible<2->{leçon}
				\item<3>[\alert{\textlang{breton}{kevrin}}] \alert{secret}
			\end{description}
		\column{0.49\textwidth}
			\begin{description}
				\item[\textlang{breton}{labourin}] \mbox{}\visible<2->{travailler-fut-1sg}
				\item[\textlang{breton}{c'hoarzhec'h}] \mbox{}\visible<2->{rire-ipfv-2pl}
				\item[\textlang{breton}{soñjont}] \mbox{}\visible<2->{penser-pres-3pl}
				\item[\textlang{breton}{graet}] \mbox{}\visible<2->{aller-pst-ptcp}
			\end{description}
	\end{columns}
\end{frame}

\againframe[standout]{kevrin}

\againframe<2>{lexicon}

\againframe[standout]{kevrin}

\begin{frame}<-2>[fragile, plain]
	\begin{center}
		\begin{tikzpicture}
			% Population 1
			\node at (-5.771476230191826,3.8908509886473874) {sardin};
			\node at (-5.731442869057548,2.9834281362704065) {gwez};
			\node at (-4.276897414512093,2.863328052867571) {reizh};
			\node at (-3.7297748123436194,3.864162081224535) {kentel};

			% Population 2
			\node at (2.4487072560467054,-1.3268304125202521) {labourin};
			\node at (2.2351959966638866,-2) {c'hoarzhec'h};
			\node at (2.6488740617180984,-2.4877978854143303) {soñjont};
			\node at (3.6363636363636362,-2.7813758670657065) {graet};

			\node[visible on={2}, color=electricindigo] at (-3, 2) {kevrin};
		\end{tikzpicture}
	\end{center}
\end{frame}

\againframe[standout]{kevrin}

\againframe<3>{lexicon}

\begin{frame}{Machine}
	Les algos d'apprentissage marchent plutôt sur des nombres ou souvent des séries de nombres
	(parce qu'on peut les voir comme des points dans un espace géométrique)  On a donc un problème
	constant qui est : 

	Comment transformer nos données (texte, signal sonore, images, vidéo) en nombres ?
\end{frame}

\begin{frame}{Méthodes immédiates}
	En soi on a toujours une solution : \pause de fait, si on a des données sous forme numérique (dans une
	machine), elles sont comme leur nom l'indique numérique (représentées par des nombres, vous
	suivez ?)

	Ces représentations peuvent être bien (par exemple le codage numérique des images traduit une
	réalité physique/perceptive du coup, c'est pas mal) ou pas du tout (une suite de nombres
	représentant des points de code Unicode qui représentent des caractères, c'est assez éloigné de la
	façon dont on pense le langage).

	\pause

	Ça ne veut pas dire que c'est impossible de se servir de ça, mais plutôt que c'est pas toujours
	la meilleure solution. Ça dépend de beaucoup de facteurs.
\end{frame}

\begin{frame}[standout]
	Comment représenter des données linguistiques écrites ?
\end{frame}

\begin{frame}{Représenter les mots}
	On peut représenter les mots comme des nombres en constituant un \alert{lexique} et en affectant
	à chaque item un nombre entier : sa position dans l'ordre lexicographique.

	\pause

	\begin{description}
		\item[a] \num{0}
		\item[aa] \num{1}
		\item[abaca] \num{2}
		\item[abaissa] \num{3}
		\item[…] … 
	\end{description}

	\pause

	Alors un mot, c'est juste un nombre et une suite de mots (phrase, tour de parole, document…)
	c'est juste une suite de nombres.
\end{frame}

\begin{frame}[standout]
	Est-ce que vous voyez des problèmes ?
\end{frame}

\begin{frame}{Une affaire de topologie}
	Un problème assez problématique, c'est que ces représentations numériques sont purement
	\alert{orthographiques}, ce qui n'est pas idéal.

	\pause

	Déjà, c'est un nid à problèmes liés à la standardisation.

	\pause

	Mais même si on en fait abstraction :

	\begin{description}
		\item[…] …
		\item[défiés] \num{41921}
		\item[déflagration] \num{41922}
		\item[…] … 
	\end{description}

	\pause

	On a des mots qui n'ont presque rien en commun, mais dont les \alert{représentations} sont très
	proches.

	\pause

	Ça sera un problème pour beaucoup des algorithmes d'apprentissage.
\end{frame}

\begin{frame}<-4>[fragile]{Une solution de mathématicien⋅ne} Il y a une façon de représenter un
	lexique à peine plus compliquée qui évite ce problème : un encodage
	\quoteforeign{english}{\alert{one-hot}} : on représente chaque mot par un \alert{vecteur} avec
	des zéros partout, sauf à sa position dans le lexique où on met un \num{1}.

	\pause

	Par exemple \enquote{chat} est en position \num{2332}, on va donc le représenter par

	\begin{equation}
		\vcord{\mathnode{vectop}{0},0,⋮,0,\mathnode{one}{1},0,⋮,\mathnode{vecbottom}{0}}
	\end{equation}
	\tikz[overlay, remember picture]{
		\draw[->] ($(one)+(2em, 0)$) node[right] {\num{2332}} -- (one);
		\draw[visible on={4-}, decorate, decoration={brace, mirror, amplitude=1ex}]
			($(vectop.north)+(-2ex, 0)$) -- ($(vecbottom.south)+(-2ex, 0)$) node [midway, xshift=-1ex, anchor=east] 
			{\num{142695}}
		;
	}
\end{frame}

\begin{frame}{Une solution de mathématicien⋅ne}
	Dans cette configuration, quelle est la distance (euclidienne) entre deux mots ?

	\pause

	Est-ce que ça résout le problème précédent ?

	\pause

	Est-ce que vous voyez d'autres problèmes avec cette représentation ?
\end{frame}

\begin{frame}[standout]
	Des idées pour faire mieux ?
\end{frame}

\begin{frame}{Une solution de linguiste}
	Si on dispose d'un lexique annoté (au hasard JeuxDeMots
	\parencite{lafourcade2020JeuxDeMotsReseauLexicosemantique}), on peut aller dans la direction
	carrément opposée : représenter chaque mot par une \alert{structure de traits linguistiques}.

	\pause

	\begin{equation}
		\vcord{\text{NOUN},\text{singular},\text{animated},⋮}
	\end{equation}

	\pause

	Est-ce que ça résout nos problèmes précédents ?

	\pause

	Est-ce que ça en créé d'autres ?
\end{frame}

\begin{frame}{Une solution de linguiste \emph{de corpus}}

	\begin{center}
		{\large\quoteforeign{english}{\alert{You shall know a word by the company it keeps.}}}
		
		\parencite{firth1957SynopsisLinguisticTheory}
	\end{center}

	\pause

	\vfill
	{\footnotesize Voir par exemple \textcite{brunila2022WhatCompanyWords} pour une mise en contexte plutôt nécessaire.}
\end{frame}

\begin{frame}{Une solution de langouste}
	Bien entendu on va s'empresser d'oublier toute nuance sur cette citation et d'en tirer une idée
	pas forcément parfaite mais \alert{pratique}.

	\pause

	On va faire l'hypothèse sauvage que des mots qui apparaissent dans des \alert{contextes}
	similaires ont des propriétés similaires.

	\pause

	Ça nous donne une solution opérante pour représenter des mots : on va les représenter par les
	contextes dans lesquels ils apparaissent.

	\pause

	En général : leurs fréquences de cooccurrence avec chacun des mots d'un lexique dans un grand
	corpus.
\end{frame}

\begin{frame}{Cooccurrences}
	Représenter un mot par des fréquences de cooccurrences, ça donne toujours des vecteurs de la
	taille du lexique, \alert<1>{mais} :

	\pause

	\begin{itemize}[<+->]
		\item Beaucoup moins creux (moins de zéros).
		\item Dont la \alert{topologie}, l'organisation spatiale, a plus d'intérêt.
	\end{itemize}

	\pause

	La grande dimension est toujours un problème, mais il y a des techniques pour la réduire,
	\alert{compresser} les vecteurs en gardant, voire améliorant l'intérêt de leur
	\alert{géométrie}.
\end{frame}

\begin{frame}[standout]
	Quel rapport avec l'apprentissage automatique ?
\end{frame}

\begin{frame}{Apprendre des représentations}
	On a ici \alert{appris} des représentations : on a exploité des \alert{régularités} repérées
	dans un \alert{échantillon de données}.

	\pause

	On l'a fait ici avec un mécanisme très \emph{ad hoc}, mais il y a d'autres solutions.

	\pause

	Notamment la plus populaire et celle qui va nous préoccuper maintenant : utiliser des
	représentations apprises pour une \alert{tâche auxiliaire} pour laquelle les données abondent.
\end{frame}

\begin{frame}[fragile]{???}
	En fait, ce qui se passe dans beaucoup de modèles d'apprentissage, ça ressemble à

	\begin{figure}
        \begin{tikzpicture}
            \graph[grow right sep, nodes={ellipse, draw}]{
				Entrée -> Représentation -> Sortie
			};
        \end{tikzpicture}
	\end{figure}

	\pause

	Le modèle apprend à la fois une transformation des entrées en des \alert{représentations
	intermédiaires} pertinentes pour lui et à déterminer une sortie à partir de ces représentations.

	\pause

	Si on applique un tel modèle à une tâche qui demande des représentations intéressantes et pour
	laquelle on a facilement des données, on pourra alors extraire ces représentations et les
	utiliser pour d'autres tâches.
\end{frame}

\begin{frame}{⚠️}
	Évidemment ce que vous obtenez dépend très fortement de vos données et de la tâche.

	\pause

	Les données faciles à obtenir c'est bien pratique, mais c'est aussi dangereux (on en a déjà
	parlé, on en reparlera etc).
\end{frame}

\begin{frame}{🐍}
	Est-ce que le serpent ne se mord pas un peu la queue ?

	\pause

	Pour cette tâche auxiliaire, on prend quoi comme représentation de départ ?

	\pause

	En fait on peut prendre un peu ce qu'on veut tant qu'on perd pas d'information : on est de toute
	façon pas très intéressé⋅e par le modèle de la tâche auxiliaire en soi. Tout ce qu'on veut,
	c'est qu'elle apprenne des bonnes représentations intermédiaires.

	\pause

	En pratique : souvent des vecteurs \emph{one-hot} pour les mots. On pourrait faire du bootstrap,
	mais à ma connaissance ça ne se fait pas vraiment en pratique.
\end{frame}

\begin{frame}{Tâches auxiliaires}
	Quelles tâches ?

	\pause

	\begin{itemize}[<+->]
		\item Suffisamment complexe.
		\item Facilement beaucoup de données.
	\end{itemize}

	\pause

	Le plus populaire en TAL, ce sont des formes de \enquote{\alert{modèles de langue}}.

	\pause

	Par exemple génératif : \enquote{Le petit chat est
	\textunderscore\textunderscore\textunderscore} ← quelle est la proba pour chacun des mots du
	vocabulaire de continuer la phrase ?

	\pause

	C'est historiquement ce que fait \textcite{bengio2006NeuralProbabilisticLanguage}.
	
	\pause
	
	C'est ce que fait \textcite{mikolov2013EfficientEstimationWord} avec plusieurs modèles de
	vraisemblance partielles (en regardant en biais).
\end{frame}

\begin{frame}{Ces fameux \emph{embeddings}}
	Ce que \textcite{bengio2006NeuralProbabilisticLanguage},
	\textcite{mikolov2013EfficientEstimationWord} et les autres obtiennent ce sont des
	\alert{représentations vectorielles} de \alt<+>{mots}{\strikeThrough{mots} formes
	orthographiques}.

	\pause

	Fondamentalement, on a un \alert{dictionnaire} qui associe à chaque forme orthographique un
	vecteur qui contient des \strikeThrough{trucs} informations sur son usage observé dans un
	corpus.

	\pause

	On les appelle \alert{embeddings} (ou \emph{plongements}) pour des raisons. Ce sont des vecteurs
	denses (peu de zéros), de dimensions très inférieures à la taille du lexique. Tout ce que les
	algos d'apprentissage (en particulier les réseaux de neurones) aiment bien.

	\pause

	C'est difficile de les évaluer intrinsèquement (même si
	\citeauthor{mikolov2013EfficientEstimationWord} propose des idées sur les liens
	géométrie-sémantique), mais on observe en pratique que \alert{pré-entraîner} ainsi des
	représentations aide pour des tâches en aval.
\end{frame}

\begin{frame}{La tuyauterie des données}
	Si on revient à notre objectif initial \pause qui était ?

	\pause

	Quelles représentations pour nos données dans un algo d'apprentissage pour une \alert{tâche}
	précise ?

	\pause

	Une solution très orthodoxe : un problème d'apprentissage ? \pause Résolvons-le avec deux fois
	plus d'apprentissage :
	
	\pause

	\begin{itemize}[<+->]
		\item Une première phase exploite des \alert{régularités} sur des données qui n'ont pas été
		annotées pour cette tâche. Nombreuses, \enquote{bon marché} et peu spécifiques \pause : par
		exemple un corpus de texte brut genre OSCAR
		\parencite{ortizsuarez2019AsynchronousPipelineProcessing} et en apprend ce faisant des
		représentations.
		\item La deuxième apprend des corrélations entre \alert{ces} représentations et des
		informations contenues dans des données \alert{annotées spécifiquement} \pause : par exemple
		un treebank comme Sequoia \parencite{candito2012CorpusSequoiaAnnotation}.
	\end{itemize}
\end{frame}

\begin{frame}[standout]
	Opinions ?
\end{frame}

\begin{frame}{Une note}
	Si pour plusieurs tâches aval on a l'intention d'utiliser le même pré-entraînement de
	représentations, on est pas obligé de recommencer à chaque fois !

	\pause

	On a donc potentiellement des représentations \alert{polyvalentes}.

	\pause

	Et ça permet de faire des économies assez importantes.
\end{frame}

\begin{frame}{Encore des problèmes}
	Ces représentations de formes, ça marche \strikeThrough{bien} pas si pire pour beaucoup de
	choses, mais elles ont défaut majeur.

	\pause

	Ce sont des représentations de \alert{formes}.

	\pause

	Les homo(graphes|nymes) ça existe.

	\pause

	Comment faire pour que deux formes identiques de lexèmes différents, ou issues d'un syncrétisme
	morphologique d'un même lexème aient des représentations différentes ?
\end{frame}

\begin{frame}{Désambigüiser}
	Peut-être on pourrait au préalable désambigüiser ?

	\pause

	Comment ?

	\pause

	{\footnotesize Avec un modèle appris ? 👉👈}
\end{frame}

\begin{frame}{Désambigüiser}
	Le plan ça pourrait être :

	\begin{enumerate}[<+->]
		\item On apprend des représentations de formes
		\item On les utilise comme entrées dans un modèle qui saurait désambigüiser des formes
		\item Le modèle qui réalise notre tâche cible finale a donc accès à des représentations des
		      formes, sans les ambigüités
	\end{enumerate}

	\pause

	Est-ce que ça vous donner une idée lumineuse ?
\end{frame}

\begin{frame}{Wavelength collapse}
	Un modèle de désambigüisation va désambigüiser en utilisant des représentations des formes
	\alert{en contexte}.

	\pause

	Il va donc y avoir dedans des représentations de chacune des formes \alert{dans son contexte}.

	\pause

	C'est encore mieux qu'une désambigüisation arbitraire : si on a de la chance ça pourrait
	capturer des nuances qu'on a pas annoté explicitement.

	\pause

	Le plan ça pourrait donc être de prendre directement ces représentations : quand on nous donne
	une entrée, on la passe au modèle de désambigüisation, on intercepte ses représentations
	contextuelles internes et on se sert de \alert{ça} comme entrée pour notre tâche cible.

	\pause

	Et qu'est-ce que vous pensez de ce qui se passe dans un \alert{modèle de langue} ?
\end{frame}

\begin{frame}{Wavelength collapse}
	Un très bon modèle de langue devrait contenir aussi des représentations en contexte
	
	\pause
	
	Et il doit probablement faire une forme de désambigüisation.

	\pause

	Alors pourquoi ne pas shunter la désambigüisation et utiliser directement ces
	\alert{représentations contextuelles}-là ?

	\pause

	C'est exactement ce qui s'est produit avec quasi-simultanément
	\textcite{devlin2019BERTPretrainingDeep, howard2018UniversalLanguageModel,
	peters2018DeepContextualizedWord, radford2018ImprovingLanguageUnderstanding}

	(BERT, ULMFit, ELMo, GPT : diverses formes d'entraînement de représentations à partir de formes
	de modèles de langue)

\end{frame}

\begin{frame}{Killing the game}
	Les représentations issues desdits modèles ont pris d'assaut le TAL, la linguistique
	informatique, les esprits et la société.

	\pause

	Il faut dire qu'elles apportent des gains impressionnants de performances (lesquelles ?) \pause
	à un coût computationnel défiant toute concurrence \pause (monstrueux). \pause Mais peut-être
	acceptable (???) selon ce qu'on veut faire.

	\pause

	De fait, ce qui se passe : quand on veut une représentation vectorielle d'un mot (dans un
	contexte) on a plus seulement à aller la chercher dans un \alert{dictionnaire}, mais on doit à
	chaque fois faire passer tout le contexte dans un \alert{modèle} qui peut être très lourd.
 \end{frame}

\begin{frame}{Transfert}
	Si on fait un pas en arrière sur ce dont on vient de parler, du point de vue de
	l'apprentissage :

	\pause

	\begin{itemize}[<+->]
		\item On a appris un modèle pour une certaine tâche.
		\item Puis on prend ce modèle, on lui ajoute une surcouche et on entraîne l'ensemble sur une
		      nouvelle tâche.
		\item D'une certaine façon, on a \alert{transféré} des connaissances d'une tâche vers une
		      autre.
	\end{itemize}
\end{frame}

\begin{frame}{\textlang{english}{\emph{Transfer learning}}}
	Cette idée est parallèle à celle de l'\alert{apprentissage par transfert} en général.

	\pause

	Pour le dire vite : si on a un modèle qui marche bien pour un contexte \(X\), c'est peut-être
	moins coûteux et plus efficace de l'\enquote{\alert{adapter}} pour un contexte \(Y\) que de
	faire un modèle pour \(Y\) à partir de zéro.

	\pause

	Ça a été essayé pour plein de combinaisons de \(X\) et \(Y\)
	(\textcite{ruder2019NeuralTransferLearning} pour un bon historique).

	\begin{itemize}
		\item D'une langue à une autre.
		\item Entre domaines de spécialité.
		\item Entre tâches proches (POS/parsing/…)
	\end{itemize}
\end{frame}

\begin{frame}{Adaptation}
	\alert{Adapter} un modèle concrètement ça peut être

	\pause

	\begin{itemize}[<+->]
		\item Juste prendre le modèle de base et lui donner de nouvelles données.
			\begin{itemize}
				\item[→] Faisable aussi pour le modèle de représentation !
			\end{itemize}
		\item Ajouter des sur-/sous-/inter-couches dans le modèle qui gèrent l'adaptation :
			\begin{itemize}
				\item[→] En continuant à entraîner les paramètres du modèle original
				         (\quoteforeign{english}{\alert{fine-tuning}}).
				\item[→] En fixant les paramètres du modèle original et juste entraîner les
				         nouvelles parties.
			\end{itemize}
	\end{itemize}

	\pause

	On peut aussi envisager plusieurs entraînements simultanés, en \alert{multi-tâches} (en partageant des représentations), c'est d'ailleurs déjà ce que faisait \textcite{bengio2006NeuralProbabilisticLanguage}.
\end{frame}

\begin{frame}{Cas pratique : détection des chaînes de coréférences}
	\begin{center}
		\begin{tabular}{c*{5}{S}}
			\toprule
			{\textbf{System}}	& {\textbf{MUC}}	& {\textbf{B³}}	& {\textbf{CEAF\textsubscript{e}}}	& {\textbf{CoNLL}}	& {\textbf{BLANC}}\\
			\midrule
			No pretraining & 62.15 & 81.24 & 81.29 & 74.89 & 69.50\\
			frELMo & 67.06 & 82.53 & 83.56 & 77.72 & 71.74\\
			mBERT & 64.02 & 81.67 & 82.40 & 76.03 & 70.42\\
			CamemBERT & 67.32 & 82.53 & 83.63 & 77.83 & 71.96\\
			frELMo+str & \textbf{72.50} & 84.24 & 86.21 & \textbf{80.98} & \textbf{74.15}\\  % 2019-10-09-ANCOR-frELMOv0.3+str
			CamemBERT+str & 72.34 & \textbf{84.42} & \textbf{86.37} & \textbf{81.04} & \textbf{74.24}\\  % 2019-10-09-ANCOR-frELMOv0.3+str
			\bottomrule
		\end{tabular}
	\end{center}
\end{frame}

\begin{frame}{Cas pratique : analyse syntaxique de l'ancien français}
	\begin{center}
		\begin{tabular}{l*{3}{S[table-format=2.2]}}
			\toprule
			{\textbf{Model}} & {\textbf{UPOS}} & {\textbf{UAS}} & {\textbf{LAS}}\\
			\midrule
			SotA & 96.26 & 91.83 & 86.75\\
			\midrule
			mBERT	& 96.19 & 92.03 & 87.52\\
			BERTrade      & 96.60 & 92.20 & 87.95\\
			mBERT+OF      & 97.11 & 93.86 & 90.37\\
			FlauBERT+OF   & 97.15 & 93.96 & 90.57\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{frame}

\begin{frame}{Cas pratique : traduction Breton→Français}
	\begin{center}
		\begin{tabular}{l*{2}{S[table-format=2.2]}S[table-format=3.2]}
			\toprule
			{\textbf{Model}} & {\textbf{BLEU}} & {\textbf{ChrF++}} & {\textbf{TER}}\\
			\midrule
			Apertium            & 24.15 & 50.23 &  63.93\\
			m2m100-418M         & 00.58 & 11.85 & 114.49\\
			\quad +OPUS         & 30.01 & 50.16 &  55.37\\
			\quad\quad +ARBRES  & 37.68 & 56.99 &  48.65\\
			\bottomrule
		\end{tabular}
	\end{center}
\end{frame}


%  █████  ██████  ██████  ███████ ███    ██ ██████  ██ ██   ██
% ██   ██ ██   ██ ██   ██ ██      ████   ██ ██   ██ ██  ██ ██
% ███████ ██████  ██████  █████   ██ ██  ██ ██   ██ ██   ███
% ██   ██ ██      ██      ██      ██  ██ ██ ██   ██ ██  ██ ██
% ██   ██ ██      ██      ███████ ██   ████ ██████  ██ ██   ██

\hypersetup{bookmarksdepth=0}  % Don't create the bookmark for the Appendix part
\appendix
\hypersetup{bookmarksdepth=2}
\bookmarksetup{startatroot}
\section{Appendix}

\pdfbookmark[3]{References}{references}
\begin{frame}[allowframebreaks]{References}
	\printbibliography[heading=none]
\end{frame}

\pdfbookmark[3]{Licence}{licence}
\begin{frame}{Licence}
	\begin{english}
		\begin{center}
			{\huge \ccby}
			\vfill
			This document is distributed under the terms of the Creative Commons Attribution 4.0 International Licence (CC BY 4.0) (\shorturl{creativecommons.org/licenses/by/4.0})

			\vfill
			© 2024, L. Grobol <\shorturl[mailto][:]{loic.grobol@gmail.com}>

			\shorturl[https]{lgrobol.eu}
		\end{center}
	\end{english}
\end{frame}

\end{document}
