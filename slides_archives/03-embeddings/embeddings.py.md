---
jupyter:
  jupytext:
    formats: ipynb,md
    split_at_heading: true
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.14.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- LTeX: language=fr -->

<!-- #region slideshow={"slide_type": "slide"} -->
Cours 11â€¯: ReprÃ©sentations lexicales vectorielles
=================================================

**LoÃ¯c Grobol** [<lgrobol@parisnanterre.fr>](mailto:lgrobol@parisnanterre.fr)

2023-12-13
<!-- #endregion -->

```python
from IPython.display import display
```

## ReprÃ©sentaquoiâ€¯?

**ReprÃ©sentations lexicales vectorielles**, ou en Ã©tant moins pÃ©dantâ‹…e Â«â€¯reprÃ©sentations
vectorielles de motsâ€¯Â». Comment on reprÃ©sente des mots par des vecteurs, quoi.


Mais qui voudrait faire Ã§a, et pourquoiâ€¯?


Tout le monde, et pour plein de raisons


On va commencer par utiliser [`gensim`](https://radimrehurek.com/gensim), qui nous fournit plein de
modÃ¨les tout faits.

```python
%pip install -U gensim
```

et pour dÃ©marrer, on va tÃ©lÃ©charger un modÃ¨le tout fait

```python
import gensim.downloader as api
wv = api.load("glove-wiki-gigaword-50")
```

OK, super, qu'est-ce qu'on a rÃ©cupÃ©rÃ©â€¯?

```python
type(wv)
```

C'est le bon moment pour aller voir [la
doc](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors).
On y voit qu'il s'agit d'un objet associant des mots Ã  des vecteurs.

```python
wv["monarch"]
```

Des vecteurs stockÃ©s commentâ€¯?

```python
type(wv["monarch"])
```

Ah parfait, on connaÃ®t : c'est des tableaux numpy

```python
wv["king"]
```

```python
wv["queen"]
```

D'accord, trÃ¨s bien, on peut faire quoi avec Ã§aâ€¯?


Si les vecteurs sont bien faits (et ceux-ci le sont), les vecteurs de deux mots Â«â€¯prochesâ€¯Â»
devraient Ãªtre proches, par exemple au sens de la similaritÃ© cosinus

```python
import numpy as np
def cosine_similarity(x, y):
    """Le cosinus de l'angle entre `x` et `y`."""
    return np.inner(x,y)/(np.linalg.norm(x)*np.linalg.norm(y))
```

```python
cosine_similarity(wv["monarch"], wv["king"])
```

```python
cosine_similarity(wv["monarch"], wv["cat"])
```

En fait le modÃ¨le nous donne directement les mots les plus proches en similaritÃ© cosinus.

```python
wv.most_similar(["monarch"])
```

Mais aussi les plus Ã©loignÃ©s

```python
wv.most_similar(negative=["monarch"])
```

### ğŸ§¨ Exo ğŸ§¨

1\. Essayez avec d'autres mots. Quels semblent Ãªtre les critÃ¨res qui font que des mots sont proches
dans ce modÃ¨le.

2\. Comparer avec les vecteurs du modÃ¨le `"glove-twitter-100"`. Y a-t-il des diffÃ©rencesâ€¯?
**Note**â€¯: il peut Ãªtre long Ã  tÃ©lÃ©charger, commencez par Ã§a.

3\. EntraÃ®ner un modÃ¨le [`word2vec`](https://radimrehurek.com/gensim/models/word2vec.html) avec
gensim sur les documents du dataset 20newsgroups. Comparer les vecteurs obtenus avec les prÃ©cÃ©dents,
par exemple en traÃ§ant un histogramme des distances entre les diffÃ©rentes reprÃ©sentations d'un mÃªme
mot.

â†’ Vous pouvez rÃ©cupÃ©rer 20newgroups directement [sur sa page](http://qwone.com/~jason/20Newsgroups/)
ou [via
scikit-learn](https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset).

## SÃ©mantique lexicale distributionnelle

### Principe gÃ©nÃ©ral

Pour le dire viteâ€¯:

La *sÃ©mantique lexicale*, c'est l'Ã©tude du sens des mots. Rien que dire Ã§a, c'est dÃ©jÃ  faire
l'hypothÃ¨se hautement non-triviale que les mots existent et ont un (ou plus vraisemblablement des)
sens.

C'est tout un pan de la linguistique et on ne rentrera pas ici dans les dÃ©tails (mÃªmes s'il sont
passionnantsâ€¯!) parce que notre objectif est *applicatif*â€¯:

- Comment reprÃ©senter le sens d'un motâ€¯?
- Peut-on, Ã  partir de donnÃ©es linguistiques, dÃ©terminer le sens des motsâ€¯?
- Et plus tardâ€¯: comment on peut s'en servirâ€¯?

Une faÃ§on de traiter le problÃ¨me, c'est de recourir Ã  de l'annotation manuelle (par exemple avec
[Jeux de mots](http://www.jeuxdemots.org), d'ailleurs, vous avez jouÃ© rÃ©cemmentâ€¯?).

On ne se penchera pas plus dessus iciâ€¯: ce qui nous intÃ©resse, c'est comment traiter ce problÃ¨me
avec de l'apprentissage, et en particulier avec de l'apprentissage sur des donnÃ©es non-annotÃ©es.

Pour Ã§a, la faÃ§on la plus populaire (et pour l'instant celle qui semble la plus efficace) repose sur
l'**hypothÃ¨se distributionnelle**, formulÃ©e ainsi par Firthâ€¯:

<!-- LTeX: language=en-GB -->
> *You shall know a word by the company it keeps.*
<!-- LTeX: language=fr -->

Autrement ditâ€¯: des mots dont le sens est similaire devraient apparaÃ®tre dans des contextes
similaires et vice-versa.


Si on pousse cette hypothÃ¨se Ã  sa conclusion naturelleâ€¯: on peut reprÃ©senter le sens d'un mot par
les contextes dans lesquels il apparaÃ®t.

Le principal dÃ©faut de cette vision des choses, c'est que ce n'est pas forcÃ©ment trÃ¨s interprÃ©table,
contrairement par exemple Ã  des reprÃ©sentations en logique formelle. Mais Ã§a nous donne des moyens
trÃ¨s concrets d'apprendre des reprÃ©sentations de mots Ã  partir de corpus non annotÃ©s.

### ModÃ¨le par documents

Par exemple une faÃ§on trÃ¨s simple de l'appliquer, c'est de regarder dans quels documents d'un grand
corpus apparaÃ®t un motâ€¯: des mots qui apparaissent dans les mÃªmes documents avec des frÃ©quences
similaires devraient avoir des sens proches.

Qu'est-ce que Ã§a donne en pratiqueâ€¯? Et bien, souvenez-vous du modÃ¨le des sacs de motsâ€¯: on peut
reprÃ©senter des documents par les frÃ©quences des mots qui y apparaissent. Ã‡a nous donne une
reprÃ©sentation vectorielle d'un corpus sous la forme d'une matrice avec autant de ligne que de
documents, autant de lignes que de mots dans le vocabulaire et oÃ¹ chaque cellule est une frÃ©quence.

Jusque-lÃ  on s'en est servi en lisant les lignes pour rÃ©cupÃ©rer des reprÃ©sentations vectorielles des
documents, mais si on regarde les colonnes, on rÃ©cupÃ¨re des **reprÃ©sentations vectorielles des
mots**â€¯!

(Ce qui rÃ©pond Ã  la premiÃ¨re questionâ€¯: comment reprÃ©senter le sensâ€¯? Comme le reste, avec des
vecteursâ€¯!)

### ğŸ¢ Exo ğŸ¢

Ã€ partir du corpus 20newsgroups, construire un dictionnaire associant chaque mot du vocabulaire Ã 
une reprÃ©sentation vectorielle donnant ses occurrences dans chacun des documents du corpus.

Est-ce que les distances entre les vecteurs de mots ressemblent Ã  celles qu'on observait avec
Gensimâ€¯?

Est-ce que vous voyez une autre faÃ§on de rÃ©cupÃ©rer des vecteurs de mots en utilisant ce corpusâ€¯?


### Cooccurrences

Une autre possibilitÃ©, plutÃ´t que de regarder dans quels documents apparaÃ®t un mot, c'est de
regarder directement les autres mots dans son voisinage. Autrement dit les cooccurrences.

L'idÃ©e est la suivanteâ€¯: on choisit un paramÃ¨tre $n$ (la Â«â€¯taille de fenÃªtreâ€¯Â») et on regarde pour
chaque mot du corpus les $n$ mots prÃ©cÃ©dents et les $n$ mots suivants. Chacun de ces mots voisins
constitue une cooccurrence. Par exemple avec une fenÃªtre de taille $2$, dansâ€¯:

> Le petit chat est content

On a les cooccurrences `("le", "petit")`, `("le", "chat")`, `("petit", "chat")`, `("petit", "est")`â€¦

Comment on se sert de Ã§a pour rÃ©cupÃ©rer une reprÃ©sentation vectorielle des motsâ€¯? Comme
d'habitudeâ€¯: on compteâ€¯! Ici on reprÃ©sentera chaque mot par un vecteur avec autant de coordonnÃ©es
qu'il y a de mots dans le vocabulaire, et chacune de ces coordonnÃ©es sera le nombre de cooccurrences
avec le mot correspondant.


### ğŸ¦˜ Exo ğŸ¦˜

Ã€ partir du corpus 20newsgroups, construire un dictionnaire associant chaque mot du vocabulaire Ã 
une reprÃ©sentation vectorielle par la mÃ©thode des cooccurrences pour une taille de fenÃªtre choisie.

Est-ce que les distances entre les vecteurs de mots ressemblent Ã  celles qu'on observait avec les
reprÃ©sentations prÃ©cÃ©dentesâ€¯?

## Extensions

Le dÃ©faut principal de ces reprÃ©sentations, c'est qu'elles sont trÃ¨s **creuses**â€¯: beaucoup de
dimensions, mais qui contiennent surtout des zÃ©ros. Ce n'est pas trÃ¨s Ã©conomique Ã  manipuler et
c'est moins utile quand on veut les utiliser comme entrÃ©e pour des systÃ¨mes de TAL, comme des
rÃ©seaux de neurones

L'essentiel du travail fait ces dix derniÃ¨res annÃ©es dans ce domaine consiste Ã  trouver des
reprÃ©sentations **denses**â€¯: moins de dimensions (au plus quelques centaines) mais peu de zÃ©ros. ON
parle alors en franÃ§ais de *plongements* et en anglais de *word embeddings*.

Il y a beaucoup de faÃ§ons de faire Ã§a, *Speech and Language Processing* dÃ©taille la plus connue,
*word2vec* et je vous encourage Ã  aller voir comment Ã§a marche.

Une autre possibilitÃ© d'extensions est de descendre en dessous de l'Ã©chelle du mot, et d'utiliser
des sous-mots, qui peuvent Ã©ventuellement avoir un sens linguistique (comme des morphÃ¨mes), mais
sont eux aussi en gÃ©nÃ©ral appris de faÃ§on non-supervisÃ©e. C'est ce que fait
[FastText](https://fasttext.cc/docs/en/python-module.html), qui est plus ou moins ce qui se fait de
mieux en termes de reprÃ©sentations vectorielles de mots.

## ğŸ‘½ Exo ğŸ‘½

(Pour les plus motivÃ©â‹…es, mais la doc vous dit dÃ©jÃ  presque tout)

1\. EntraÃ®ner un modÃ¨le non-supervisÃ© [`FastText`](https://fasttext.cc/docs/en/python-module.html)
sur 20newsgroups et voir si les similaritÃ©s sont les mÃªmes que pour les modÃ¨les prÃ©cÃ©dents.

2\. EntraÃ®ner et tester un modÃ¨le de classification FastText sur 20 newsgroup.
